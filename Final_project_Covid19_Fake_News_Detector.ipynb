{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tr_jEBnh-jv"
      },
      "source": [
        "# Title: Modifying Covid19 Fake News Detector\n",
        "\n",
        "#### Group Member Names : Kam Hung Chan and Tongfei Gou\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKSxMvrh-j0"
      },
      "source": [
        "### INTRODUCTION:\n",
        "This article is a research effort on COVID-19 fake news. With the COVID-19 pandemic, people are fighting not only the epidemic, but also the deluge of information, the so-called “infoplague”. Social media is awash with fake news and rumors, and believing these rumors can cause significant harm, especially during a pandemic. To address this challenge, Patwa, Sharma, Pykl, Guptha, Kumari, Akhtar,  Ekbal, Das, and Chakraborty(2020) curated and published a manually labeled dataset of 10,700 social media posts and articles about COVID-19, covering both real and fake news. The authors conducted a binary classification task (real vs fake) and benchmarked it on a labeled dataset using four machine learning baselines (decision trees, logistic regression, gradient boosting, and support vector machines). In this project, we have successfully replicated the code and results, experiment with changing the four models default parameters to create another four models with better performance. Lastly, we have found an unseen dataset from Kaggle created by Banik(2020) to test both original models and modified ones. \n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### AIM :\n",
        "The main purpose of this study is to deal with the spread of false news on social media during the COVID-19 epidemic. By modifying and testing the fake news detector, the dataset containing real and fake news and benchmarking it on a binary classification task has been used, followed the researchers' work, we aimed to provide a improvement of the models to identify and differentiate between real and disinformation on social media. This helps the public better understand and distinguish information on social media and reduce the spread of false information, thereby reducing misleading and harm.\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### Github Repo: https://github.com/Karlis00/covid19-fake-news-detector\n",
        "\n",
        "*********************************************************************************************************************\n",
        "#### DESCRIPTION OF PAPER:\n",
        "This text outlines the problem of fake news and rumors spreading alongside the COVID-19 pandemic, often exacerbated by the vast reach of social media platforms. Belief in such misinformation can lead to significant harm, particularly during a crisis. To address this issue, the authors introduce a dataset containing 10,700 social media posts and articles labeled as real or fake news related to COVID-19.(Patwa, Sharma, Pykl, Guptha, Kumari, Akhtar,  Ekbal, Das, and Chakraborty, 2020) They focus on a binary classification task of distinguishing between real and fake content. The dataset is curated from various sources, including fact-checking websites and verified Twitter accounts. The authors also conduct exploratory data analysis and implement four machine learning baselines to benchmark the dataset.\n",
        "*********************************************************************************************************************\n",
        "#### PROBLEM STATEMENT :\n",
        "The abstract and introduction underscore the grave issue of misinformation, particularly fake news, proliferating alongside the COVID-19 pandemic, contributing to what is termed an 'infodemic'. This misinformation poses significant risks as it can lead to harmful actions or decisions when believed. \n",
        "*********************************************************************************************************************\n",
        "#### CONTEXT OF THE PROBLEM:\n",
        "* The background to the problem is the proliferation of misinformation on social media during the COVID-19 pandemic. As the epidemic spreads, the number of false news and rumors posted by people on social media has increased, further exacerbating the so-called infodemic. False information may lead people to make wrong decisions or take harmful actions, posing a serious threat to public health and social stability. Therefore, solving the problem of disinformation has become particularly urgent, and measures need to be taken to identify, prevent and eliminate the spread of such misinformation to maintain public safety and trust.\n",
        "*********************************************************************************************************************\n",
        "#### SOLUTION:\n",
        "* The solution is to combat the proliferation of misinformation on social media during the COVID-19 pandemic by building a manually annotated dataset of 10,700 social media posts and articles covering real and fake news about COVID-19. The authors conducted a binary classification task to distinguish real from fake news and benchmarked the dataset using four machine learning benchmark models - decision trees, logistic regression, gradient boosting and support vector machines (SVM). The results show that on the test set, SVM achieved an F1 score of 93.32% and performed best. The author also provides links to the dataset and code for easy use and reference by other researchers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77PIPLQ-h-j1"
      },
      "source": [
        "# Background\n",
        "Reference:The Fake News Challenge (FNC-1) organized in 2017 aimed to create a dataset for automatic detection using machine learning and deep learning techniques. Other efforts include datasets like those collected by Vlachos and Riedel (2014) and Zubiaga et al. (2016).Additionally, researchers have explored techniques to limit the spread of misinformation, including approaches such as fake news spread prevention using multimodal attention networks (Vo and Lee, 2020). For rumour debunking, datasets like Emergent (Ferreira and Vlachos, 2016) have been created, containing claims and associated news articles annotated by journalists for veracity.Researchers have developed datasets of fact-checked news articles for COVID-19 (Shahi and Nandini, 2020) and proposed models, such as BERT-based models augmented with additional features from Twitter (Kar et al., 2020), and automated pipelines for COVID-19 fake news detection using fact-checking algorithms (Vijjali et al., 2020).\n",
        "\n",
        "Explanation:The development of the dataset containing real and fake news related to COVID-19. Real news comprises tweets from verified sources providing factual information about COVID-19, while fake news includes tweets, posts, and articles making unverified claims or speculations about the virus. The data collection process follows guidelines ensuring that the content is related to COVID-19 and is in English.\n",
        "\n",
        "Dataset and Input:The data collection process follows guidelines ensuring that the content is related to COVID-19 and is in English.The vocabulary size of the dataset is 37,505 words, with 5141 common words appearing in both real and fake news.The dataset is divided into train, validation, and test sets, maintaining a balanced distribution of real and fake news across the splits. Four machine learning algorithms are experimented with for classification: Logistic Regression (LR), Support Vector Machine (SVM) with linear kernel, Decision Tree (DT), and Gradient Boost (GDBT). All algorithms are implemented using the sklearn package, and experiments are conducted on an i7 CPU, with each run taking approximately one minute. The code is made available on GitHub.\n",
        "\n",
        "\n",
        "*********************************************************************************************************************\n",
        "\n",
        "Weakness:Future work could focus on expanding the dataset by collecting more data, providing additional context such as reasons for labeling posts as real or fake, and incorporating multilingual data. Exploring deep learning techniques as an alternative to machine learning for fake news detection is also a promising avenue for research.\n",
        "|Reference|Explanation|Dataset/Input|Weakness|\n",
        "|------|------|------|------|\n",
        "\n",
        "\n",
        "\n",
        "*********************************************************************************************************************\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deODH3tMh-j2"
      },
      "source": [
        "### Implement paper code :\n",
        "*********************************************************************************************************************\n",
        "\n",
        "*We have successfully replicated the code created by Patwa, Sharma, Pykl, Guptha, Kumari, Akhtar,  Ekbal, Das, and Chakraborty(2020), and generated the same result by using the same dataset that were extracted from the social media platform. Following four original models, which are using Logistic Regression (LR), Support Vector Machine (SVM) with linear kernel, Decision Tree (DT), and Gradient Boost (GDBT), has been implement in ml_baseline.ipynb file.\n",
        "\n",
        "SVM achieves the highest test F1 score of 93.45%, closely followed by Logistic Regression with a 92.75% F1-score. Decision Tree and Gradient Boost exhibit lower performance with F1-scores of 85.25% and 86.96%, respectively. The precision and recall values for all models are similar, indicating balanced performance across the two classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original experiment in LinearSVC() with default parameters\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', LinearSVC())\n",
        "    ])\n",
        "fit = pipeline.fit(train['tweet'],train['label'])\n",
        "print('SVM')\n",
        "print ('val:')\n",
        "pred=pipeline.predict(val['tweet'])\n",
        "print_metrices(pred,val['label'])\n",
        "plot_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
        "                      title = 'Confusion matix of SVM on val data')\n",
        "\n",
        "val_ori = pd.read_csv('original_data/original_train.csv')\n",
        "val_ori_reset = val_ori.iloc[:len(pred)]\n",
        "pred_reindexed = pred[val_ori_reset.index]\n",
        "svm_val_misclass_df = val_ori_reset[pred_reindexed != val['label']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original experiment in LogisticRegression() with default parameters\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', LogisticRegression())\n",
        "    ])\n",
        "fit = pipeline.fit(train['tweet'],train['label'])\n",
        "print('Logistic Regression')\n",
        "print ('val:')\n",
        "pred=pipeline.predict(val['tweet'])\n",
        "\n",
        "print_metrices(pred,val['label'])\n",
        "plot_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
        "                      title = 'Confusion matix of LR on val data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original experiment in GradientBoostingClassifier() with default parameters\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', GradientBoostingClassifier())\n",
        "    ])\n",
        "fit = pipeline.fit(train['tweet'],train['label'])\n",
        "print('Gradient Boost')\n",
        "print ('val:')\n",
        "pred=pipeline.predict(val['tweet'])\n",
        "\n",
        "print_metrices(pred,val['label'])\n",
        "plot_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
        "                      title = 'Confusion matix of GDBT on val data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Original experiment in tree.DecisionTreeClassifier() with default parameters\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', tree.DecisionTreeClassifier())\n",
        "    ])\n",
        "fit = pipeline.fit(train['tweet'],train['label'])\n",
        "print('Decision Tree')\n",
        "print ('val:')\n",
        "pred=pipeline.predict(val['tweet'])\n",
        "\n",
        "print_metrices(pred,val['label'])\n",
        "plot_confusion_matrix(confusion_matrix(val['label'],pred),target_names=['fake','real'], normalize = False, \\\n",
        "                      title = 'Confusion matix of DT on val data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gkHhku9h-j2"
      },
      "source": [
        "*********************************************************************************************************************\n",
        "### Contribution  Code :\n",
        "*We have modified the four models by changing the parameters. For Support Vector Machine with linear kernel, we set C = 0.9 to acheive a slightly higher performance in f1-score to 93.50%. For Logistic Regression, we set C = 10, and the 'liblinear' solver is used as the default solver has changed from 'liblinear' to 'lbfgs' in 0.22. the LR model also acheives a slightly improvement in f1-score to 93.22%. For the Gradient Boost model, a relative significant improvement is achieved by changing the hyperparameters of learning_rate=0.5, n_estimators=200, subsample=0.8, min_samples_split=10, and max_depth=7. In this way, the f1-score increases to 90.92%. For the Decision Tree model, criterion='entropy'and min_samples_split=5 are set, and achieve a slightly higher performance in f1-score to 86.17%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carried out the experiments in finding the best hyperparameter of LinearSVC() to modify the model\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', LinearSVC(C=0.9))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carried out the experiments in finding the best hyperparameter of LogisticRegression() to modify the model\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', LogisticRegression(C=10, solver='liblinear'))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carried out the experiments in finding the best hyperparameter of GradientBoostingClassifier() to modify the model\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', GradientBoostingClassifier(learning_rate=0.5, n_estimators=200, subsample=0.8, min_samples_split=10, max_depth=7))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Carried out the experiments in finding the best hyperparameter of DecisionTreeClassifier() to modify the model\n",
        "pipeline = Pipeline([\n",
        "        ('bow', CountVectorizer()),\n",
        "        ('tfidf', TfidfTransformer()),\n",
        "        ('c', tree.DecisionTreeClassifier(criterion='entropy',min_samples_split=5))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the detector is aimed to detect the fake news on COVID-19 on social media, large amount of unseen data which may not be balanced may fit to the models in real-world scenario. Testing the models with imbalanced data is a crucial to evaluate their performance. Therefore, we found a new unseen dataset from Kaggle by Banik(2020) that includes 8265 samples with 'fake' label, and 364 with 'real' label. We fit the data to the models and evaluate the performance of the models in ml_baseline-test.ipynb file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check the classes of the 'label' of the new data as the data is not well structured\n",
        "num_classes = test['label'].nunique()\n",
        "unique_classes = test['label'].unique()\n",
        "print(\"Number of unique classes in 'label':\", num_classes)\n",
        "print(\"Unique classes in 'label':\", unique_classes)\n",
        "\n",
        "### Found a new unseen dataset from Kaggle by Banik(2020), and preprocess the data by only select the data row with 'fake' and 'real' label\n",
        "test = test[(test['label'] == 'fake') | (test['label'] == 'real')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YdFCgWoh-j3"
      },
      "source": [
        "### Results :\n",
        "*******************************************************************************************************************************\n",
        "\n",
        "1. The original SVM achieves the highest test F1 score of 93.45%, and achieves slightly higher performance in f1-score to 93.50% after C = 0.9 is set. Followed by Logistic Regression, original model with a 92.75% F1-score, and a slightly improvement in f1-score to 93.22% is achieved with  C = 10 and the 'liblinear' solver parameters. Gradient Boost exhibits lower performance with F1-scores of 86.96% in original model. A relative significant improvement is acheived by changing the hyperparameters of learning_rate=0.5, n_estimators=200, subsample=0.8, min_samples_split=10, and max_depth=7 to improve the performance increasing the f1-score to 90.92%.For Decision Tree originally having a F1-score of 85.25%, the modified model with parameter of criterion='entropy'and min_samples_split=5 achieved a slightly higher performance in f1-score to 86.17%. The precision and recall values for all original and modified models are similar that show balanced performance across the fake and real data sample.\n",
        "\n",
        "2. For the performance of processing the new unseen data by Banik(2020), all 4 original and modified models maintained a good performance in identifying the fake new, and keeping high f1-scores. The original and modified SVM model acheive 93.97% and 94.11% respectively; the original and modified LR model acheive 94.55% and 94.24% respectively; the original and modified GDBT model acheive 94.09% and 93.37% respectively; the original and modified DT model acheieve 84.31% and 85.12% respectively.\n",
        "\n",
        "\n",
        "#### Observations :\n",
        "*******************************************************************************************************************************\n",
        "1. As the performance of the four original models is excellent that they have above 85% of f1-score. The models have desired prediction result in general. Therefore, changing the parameters of the models training could only acheived slightly improvement in performance.\n",
        "\n",
        "2. A relative significant improvement of modifying GDBT model with its F1-score incresed from 85.25% to 90.92% has been observated in the experiment. Firstly, by increasing the learning rate from the original value, the model might be able to converge faster and find a better optimum. Second, increasing the number of estimators (trees) allows the model to capture more complex data patterns. However, overfitting may occure if too many trees is added. More estimators help the model become more expressive and complex to capture the relationships in the data in details. Thrid, the deeper trees model also help in capturing more complex relationships and patterns in the data. At the same time, it may lead to overfitting. Lastly, reducing the subsample size and increasing the minimum samples split can on the other hand reduce the overfitting issues of the model.\n",
        "\n",
        "3. In real-world situation, imbalance data like the new data we fitted to the models in this project may fit to the model. The test of fitting imblance data show the weakness of the models, including the original and modified ones. The SVM models with a linear kernel and GBDT models tend to be more resistant to imbalanced data. The original and modified GDBT model acheive a significant better performance in the imbalanced unseen data than the training data. Both model achieved more than 93% F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3JVj9dKh-j3"
      },
      "source": [
        "### Conclusion and Future Direction :\n",
        "In conclusion, this research on COVID-19 fake news has contributed valuable insights into the efficacy of machine learning models in combating misinformation during the pandemic. Through the curation and analysis of a dataset comprising social media posts and articles about COVID-19, the study successfully replicated and extended. Experimentation with model hyperparameters yielded notable improvements, particularly with the modified Gradient Boosting Decision Tree (GBDT) model, which significantly increased its F1-score from 85.25% to 90.92%. While the original models exhibited strong performance, including in identifying fake news, their effectiveness was tested against unseen imbalanced data, showing consistent when confronted with data imbalance. This is important to have ongoing research into methods for handling imbalanced datasets and refining model parameters to enhance the reliability and effectiveness of fake news detector in real-world scenarios.\n",
        "*******************************************************************************************************************************\n",
        "#### Learnings :\n",
        "The development of the dataset containing real and fake news related to COVID-19.  the methodology for preprocessing the data, extracting features, and applying machine learning algorithms for classification. The preprocessing steps involve removing links, non-alphanumeric characters, and English stop words from the text. Feature extraction utilizes the term frequency–inverse document frequency (tf-idf) technique, which weighs the importance of words based on their frequency in a document and across the entire corpus.\n",
        "\n",
        "Changing the parameters provides us insight in the skills in modifying a model and understanding of the parameters of different algorithms machine learning algorithms for classification.\n",
        "*******************************************************************************************************************************\n",
        "#### Results Discussion :\n",
        "The precision and recall values for all models are similar, indicating balanced performance across the two classes.Machine learning algorithms are applied to benchmark the dataset, with the SVM-based classifiers, both original and modified ones, achieving the best performance, achieving more than 93% F1-score on the test set.\n",
        "\n",
        "Fine-tuning the parameters may result in better performance in train and val dataset. However,  this may lead to overfitting the models. Fitting the unseen data to the LR models and GDBT models, their performance suffer from a slightly decrease because of overfitting after fine-tuning the parameters. It train the models with high complexity and focusing on the noise of the data patterns may reduce the  generalization performance on unseen data. However, for GDBT models, the improvement of the model with modified parameters is larger than the decreasement in processing unseen data, and the F1-score remains high and acceptable.\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "#### Limitations :\n",
        "Incorporating multilingual data is challenging to ensure the performance across different languages. While languages have different structures and nuances, the models will struggle to generalize to languages that has not been trained on. The model cannot manage multilingual data, and that requires careful preprocessing and feature engineering to extract relevant information effectively.\n",
        "\n",
        "Another limitation arises when testing the model without another imbalanced dataset where the 'real' label is in the majority. This lack of balance in the testing dataset can affect the model's ability in identifying both classes. Without enough examples of the 'real' class, the model might become biased towards the majority class, leading to inaccurate predictions for the minority class.\n",
        "\n",
        "*******************************************************************************************************************************\n",
        "#### Future Extension :\n",
        "Future work could focus on, firstly, expanding the dataset by collecting more data. Inculding the data with different languages, like Chinese, Spanish, French, and Japanese, can make the model accommodate to different people arround the world. Second, exploring the testing in imbalanced dataset with 'real' label in majority. Third, providing additional context such as reasons for labeling posts as real or fake, and incorporating multilingual data. Lastly, exploring deep learning techniques as an alternative to machine learning for fake news detection is also a promising avenue for research.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATXtFdtBh-j4"
      },
      "source": [
        "# References:\n",
        "\n",
        "[1]:  Parth Patwa, Shivam Sharma, Srinivas PYKL, Vineeth Guptha, Gitanjali Kumari, Md Shad Akhtar, Asif Ekbal, Amitava Das, Tanmoy Chakraborty . 6 Nov 2020  .Fighting an Infodemic: COVID-19 Fake News Dataset.URL:https://paperswithcode.com/paper/fighting-an-infodemic-covid-19-fake-news\n",
        "\n",
        "[2]:  Sumit Banik . 13 Nov 2020  .COVID Fake News Dataset.URL:https://www.kaggle.com/datasets/thesumitbanik/covid-fake-news-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQnMSAf-h-j4"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
